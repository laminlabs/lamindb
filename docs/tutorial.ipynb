{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Jupyter Notebook](https://img.shields.io/badge/Jupyter%20Notebook-orange)](https://github.com/laminlabs/lamindb/blob/main/docs/tutorial.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage files & datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn messy files into validated, queryable datasets.\n",
    "\n",
    "```{note}\n",
    "\n",
    "This tutorial manages metadata schema-less, but `lamindb` gives you a framework for managing complex typed metadata schema-full.\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up an instance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Installation and sign-up](/setup) take no time: Run `pip install lamindb` and `lamin signup <email>` on the command line."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the CLI, create a LaminDB instance with a directory `./mydata` for storing data and a SQLite database `mydata.lndb` for managing it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!lamin init --storage ./mydata  # or \"s3://my-bucket\" or \"gs://my-bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} What else can I configure during setup?\n",
    "\n",
    "1. Instead of SQLite, use PostgreSQL: `--db postgresql://<user>:<pwd>@<hostname>:<port>/<dbname>`\n",
    "2. Beyond the core schema, use bionty and other schemas: `--schema bionty,custom1,template1`\n",
    "3. Instead of an instance name that's derived from default storage, provide a custom name: `--name myinstance`\n",
    "\n",
    "For more, see {doc}`./setup`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track a data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lamindb as ln"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the code that generated a batch of data a **transform**. It could be a pipeline, a notebook or an app upload.\n",
    "\n",
    "With {class}`~lamindb.Transform`, LaminDB maintains a registry of transforms and makes it easy to link data against them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's track the notebook that's being run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.track()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling {func}`~lamindb.track`, the notebook is automatically linked as the source of all data that's about to be saved!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} What happened under the hood?\n",
    "\n",
    "1. Imported package versions were detected\n",
    "2. Notebook metadata was detected and stored in a {class}`~lamindb.Transform` record\n",
    "3. Run metadata was detected and stored in a {class}`~lamindb.Run` record\n",
    "\n",
    ":::\n",
    "\n",
    ":::{dropdown} How do I track a pipeline instead of a notebook?\n",
    "\n",
    "```python\n",
    "transform = ln.Transform(name=\"My pipeline\", version=\"1.2.0\")\n",
    "ln.track(transform)\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{dropdown} Why should I care about tracking notebooks?\n",
    "\n",
    "Most people advocate for \"not using notebooks in production\" or similar: anything that can be a pipeline, should be a pipeline.\n",
    "\n",
    "However, much insight generated from biological data is driven by computational biologists _interacting_ with it.\n",
    "\n",
    "The human-driven rationales in this process are akin to the prose-heavy design of biological experiments documented in an ELN.\n",
    "\n",
    "Also notebook that's run a single time on specific data is not a pipeline: it's a **document** that produced an insight or some other form of data representation.\n",
    "\n",
    "Because humans are in the loop, most mistakes happen when using notebooks: {func}`~lamindb.track` helps avoiding some.\n",
    "\n",
    "(An early blog post on this is [here](https://lamin.ai/blog/2022/nbproject).)\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need some toy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a file \"./mydata/mini.csv\" in default storage\n",
    "ln.dev.datasets.file_mini_csv(in_storage_root=True)\n",
    "# a directory \"./mydata/sample_001\" in default storage\n",
    "ln.dev.datasets.dir_scrnaseq_cellranger(\"sample_001\", ln.settings.storage)\n",
    "# a file \"paradisi05_laminopathic_nuclei.jpg\" in the working directory\n",
    "ln.dev.datasets.file_jpg_paradisi05().resolve()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register a file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a {class}`~lamindb.File` object from a file that's already in its desired location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ln.File(\"./mydata/mini.csv\")  # or \"s3://my-bucket/my-folder/my-file.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} What is the File class?\n",
    "\n",
    "It manages search, queries, validation & access through metadata.\n",
    "\n",
    "Basic fields are:\n",
    "\n",
    "- `id`: a universal ID (also serves as a primary key in the underlying SQL table of the instance)\n",
    "- `key`: an optional storage key, i.e., the relative path of the file in `storage`\n",
    "- `description`: an optional string description\n",
    "- `storage`: the storage location (the root, say, an S3 bucket or network location)\n",
    "- `suffix`: the file suffix\n",
    "- `size`: the file size in bytes\n",
    "- `hash`: a hash useful to check for integrity and collisions (is this file already stored?)\n",
    "- `hash_type`: the type of the hash (usually, an MD5 or SHA1 checksum)\n",
    "- `created_at`: time of creation\n",
    "- `updated_at`: time of last update\n",
    "\n",
    "Provenance-related fields are:\n",
    "\n",
    "- `created_by`: the {class}`~lamindb.User` who created the file\n",
    "- `transform`: the {class}`~lamindb.Transform` (pipeline, notebook, instrument, app) that was run\n",
    "- `run`: the {class}`~lamindb.Run` of the transform that created the file\n",
    "\n",
    "Access the path through a property:\n",
    "\n",
    "- `path`: the file path\n",
    "\n",
    "Access the underlying data:\n",
    "\n",
    "- `load()`: load the file to memory for formats like `.parquet`, `.zarr`, and `.h5ad`\n",
    "- `backed()`: stream/query the file from the cloud\n",
    "- `stage()`: a local path to a cached object\n",
    "- `replace()`: replace the content of the file\n",
    "\n",
    "For a full reference, see {class}`~lamindb.File`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon `.save()`, file metadata is written to the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we called {func}`~lamindb.track`, we know where the file came from ({class}`~lamindb.Transform` & {class}`~lamindb.Run`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file.transform)\n",
    "print(file.run)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, one works with temporary files but ultimately wants to store them in persistent storage locations, typically in the cloud. Hence, LaminDB comes with\n",
    "\n",
    "1. a default storage location for persisting data\n",
    "2. a registry for managing storage locations: {class}`~lamindb.Storage`\n",
    "\n",
    "For the sake of this tutorial, the default storage location is a local directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.settings.storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a {class}`~lamindb.File` object for a temporary fileath and use the `key` parameter to indicate the target path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary_filepath = \"./paradisi05_laminopathic_nuclei.jpg\"\n",
    "file = ln.File(temporary_filepath, key=\"images/paradisi05_image.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon `.save()`, the file is copied (or uploaded) to the target path, and file metadata is written to the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} Why does the API look this way?\n",
    "\n",
    "It's inspired by APIs building on AWS S3.\n",
    "\n",
    "Both boto3 and quilt select a bucket (akin to default storage in LaminDB) and define a target path through a `key` argument.\n",
    "\n",
    "In [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/bucket/upload_file.html):\n",
    "```python\n",
    "# signature: S3.Bucket.upload_file(filepath, key)\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('mybucket')\n",
    "bucket.upload_file('/tmp/hello.txt', 'hello.txt')\n",
    "```\n",
    "\n",
    "In [quilt3](https://docs.quiltdata.com/api-reference/bucket):\n",
    "```python\n",
    "# signature: quilt3.Bucket.put_file(key, filepath)\n",
    "import quilt3\n",
    "bucket = quilt3.Bucket('mybucket')\n",
    "bucket.put_file('hello.txt', '/tmp/hello.txt')\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 files are in the default storage location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ln.File.tree()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database stores context for both files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.view()  # link tables in the database are not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} Are there remaining questions about storing files?\n",
    "\n",
    "Here is a {doc}`docs:faq/storage`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search or query the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can search the file directly based on the {class}`~lamindb.File` registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.search(\"nuclei image\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also query & search the file by any metadata combination.\n",
    "\n",
    "For instance, look up a user with auto-complete from the {class}`~lamindb.User` registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ln.User.lookup()  # look up users with auto-complete\n",
    "users.testuser1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the {class}`~lamindb.Transform` registry for a name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = ln.Transform.filter(name__contains=\"files & datasets\").one()\n",
    "transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these query results to filter the {class}`~lamindb.File` registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.filter(\n",
    "    created_by=users.testuser1,\n",
    "    transform=transform,\n",
    "    suffix=\".jpg\",\n",
    ").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "\n",
    "You can look up, filter & search any registry! To that end, most classes in LaminDB inherit from {class}`~lamindb.dev.Registry`.\n",
    "\n",
    "For the same registry, you can arbitrarily chain `.filter()` and `.search()` statements, e.g. `ln.File.filter(suffix=\".jpg\").search(\"my image\")`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An empty filter returns the entire registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.filter().df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more info, see: {doc}`meta`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{attr}`~lamindb.File.path` gives you a filepath:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file is in the cloud, you typically stage a cached file ({meth}`~lamindb.File.stage`) or query its data ({meth}`~lamindb.File.backed`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more info, see: {doc}`data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data lineage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view a lineage graph of a tracked file with {meth}`~lamindb.File.view_lineage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.view_lineage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a comprehensive example with data handovers from app uploads, pipelines & notebooks of multiple data types, see {doc}`docs:birds-eye`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage features & labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} Why care about features & labels?\n",
    "\n",
    "1. Finding data: Which datasets measured expression of cell marker CD14? Which characterized cell line K562? Which datasets have a test & train split? Etc.\n",
    "2. Validating data: Are there typos in feature names? Are there typos in sampled labels? Are units of features consistent? Etc.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} A perspective on contextualizing data objects\n",
    "\n",
    "We love the pydata family of data objects: `DataFrame`, `AnnData`, `pytorch.DataLoader`, `zarr.Array`, `pyarrow.Table`, `xarray.Dataset`, ...\n",
    "\n",
    "But we couldn’t find an object for linking data objects to context!\n",
    "\n",
    "So, we made `lamindb.File` and `lamindb.Dataset` to model how data objects relate to their context:\n",
    "\n",
    "- other data objects, data transformations, models, users & pipelines that performed transformations (provenance)\n",
    "- any entity of the domain in which data is generated and modeled (biology)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a batch of the Iris flower dataset (a `DataFrame`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ln.dev.datasets.df_iris_in_meter_batch1()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate & link features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use {meth}`~lamindb.File.from_df` to track this DataFrame along with its columns as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ln.File.from_df(df, description=\"Iris flower dataset batch 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features couldn't be validated and are ignored because this is an empty LaminDB instance without a single registered feature.\n",
    "\n",
    "But, all features here are meaningful and well-curated, so, let's create records for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "features = ln.Feature.from_df(df)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as we save them, they'll serve as the reference for validating data batches that we'd like to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.save(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} How to track units of features?\n",
    "\n",
    "It's easy using {class}`~lamindb.Feature.unit`. In the above example, you'd do:\n",
    "\n",
    "```python\n",
    "for feature in features:\n",
    "    if feature.type == \"float\":\n",
    "        feature.unit = \"m\"  # SI unit for meters\n",
    "        feature.save()\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create the `File` now, we'll see that features are validated based on the registry content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ln.File.from_df(df, description=\"Iris flower dataset batch 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's register the file along with its linked features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an overview of linked feature sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `slot` provides a string key to access feature sets. It's typically the accessor of feature identifiers in the data object we're validating & registering (here, a `DataFrame`).\n",
    "\n",
    "Let's use it to access all linked features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.features[\"columns\"].df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate & link labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris dataset comes with labels within the data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "species_labels = ln.Label.from_values(df[\"iris_species_name\"])\n",
    "\n",
    "species_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save them to the {class}`~lamindb.Label` registry so that they get validated going forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.save(species_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And annotate the file with the labels for feature `iris_species_name`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.add_labels(species_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get linked labels from a feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.get_labels(\"iris_species_name\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now query & search the file by whether `\"setosa\"` is linked to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.filter(labels__name=\"setosa\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to features present _within_ a data object like a `DataFrame`, a file can be labeled with external metadata.\n",
    "\n",
    "Let's label this file with `\"experiment_1\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment1 = ln.Label(name=\"experiment_1\")\n",
    "experiment1.save()\n",
    "experiment1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} Why labeling a data batch by experiment?\n",
    "\n",
    "We can then\n",
    "\n",
    "1. query all files link to this experiment\n",
    "2. model it as a confounder when we'll analyze similar data from a follow-up experiment, and concatenate data using the label as a feature in a data matrix\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also register a feature that holds experiment labels in concatenated datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Feature(name=\"experiment\", type=\"category\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.add_labels(experiment1, feature=\"experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the original feature set and the external feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the context for our file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the database content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.view(registries=[\"Feature\", \"FeatureSet\", \"Label\", \"Modality\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage datasets\n",
    "\n",
    "In simple cases we just saw, we can use files to store datasets.\n",
    "\n",
    "In more complex cases, we'd like to store collections of files and data in mutable storage backends (zarr, TileDB, DuckDB, etc.) or in SQL tables in BigQuery, Snowflake, or Postgres.\n",
    "\n",
    "Hence, we need a second central class for data storage: {class}`~lamindb.Dataset`.\n",
    "\n",
    "Let's say we have a second batch of the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "df = ln.dev.datasets.df_iris_in_meter_batch2()\n",
    "ln.File.from_df(df, description=\"Iris flower dataset batch 2\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And load both files storing separate batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = ln.File.filter(description=\"Iris flower dataset batch 1\").one()\n",
    "file2 = ln.File.filter(description=\"Iris flower dataset batch 2\").one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a sharded dataset from these two batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ln.Dataset.from_files(name=\"The combined Iris dataset\", files=[file1, file2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the sharded dataset as if it was one dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.load().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the underlying two file objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.files.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.files.list()[0].view_lineage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or see the registries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.view(registries=[\"Dataset\", \"File\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more interesting data lineage graph, let's pretend we're now running a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "pipeline = ln.Transform(name=\"Iris Postprocessor\", version=\"0.7.2\")\n",
    "ln.track(pipeline)  # create & track a pipeline\n",
    "input_files = ln.File.filter(transform__name__contains=\"files & datasets\").all()\n",
    "[file.stage() for file in input_files]  # let's load the input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use {meth}`~lamindb.File.from_dir` to create files from a directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ln.File.from_dir(\"./mydata/sample_001/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.save(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View as a tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.File.tree(\"./mydata/sample_001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or as a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.filter(key__startswith=\"sample_001/\").df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "LaminDB treats directories similar to AWS S3, as a prefix in the storage `key`, queryable with `key__startswith`.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a summary for all files ingested from this notebook (interactive exploration will be possible in the app):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[0].view_lineage()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To end this guide through basic file & metadata tracking, let's see how to update registry records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical ontologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say, we want to express that `experiment_1` belongs to project 1, we can use `.parents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project1 = ln.Label(name=\"project_1\")\n",
    "project1.save()\n",
    "experiment1.parents.add(project1)\n",
    "experiment1.view_parents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more info, see {meth}`~lamindb.dev.ParentsAware.view_parents`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate records upon creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already created a `project_1` label before, let's see what happens if we try to create it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ln.Label(name=\"project_1\")\n",
    "\n",
    "label.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of creating a new record, LaminDB will load and return the existing record from the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is no exact match, LaminDB will warn you upon creating a record about potential duplictes.\n",
    "\n",
    "Say, we spell \"project_1\" without an underscore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Label(name=\"project 1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that for every record creation, a search compares whether a similar already exists!\n",
    "    \n",
    "This is to avoid inserting duplicated records.\n",
    "\n",
    "You can switch it off (for performance gains) via `ln.settings.upon_create_search_names = False`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ln.Label.filter(name=\"project_1\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.name = \"project_1a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete records like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change default storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default storage location is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.settings.storage  # your \"working data directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change it by setting `ln.settings.storage = \"s3://my-bucket\"` and see all storage locations via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Storage.filter().df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.settings.verbosity = 3  # only show info, no hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# clean up what we wrote in this notebook\n",
    "!lamin delete --force mydata\n",
    "!rm -r mydata\n",
    "!rm paradisi05_laminopathic_nuclei.jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "nbproject": {
   "id": "NJvdsWWbJlZS",
   "parent": null,
   "pypackage": null,
   "time_init": "2023-03-09T13:30:29.239953+00:00",
   "user_handle": "testuser2",
   "user_id": "bKeW4T6E",
   "user_name": "Test User2",
   "version": "0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae1fefc8646a06dd2e75004cd934adda7c5727b046986a772e3b44b0ffba9754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
