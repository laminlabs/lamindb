{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage files & datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you'll learn LaminDB's basic data management workflow.\n",
    "\n",
    "Starting with a [lake](https://en.wikipedia.org/wiki/Data_lake) of files, you'll arrive at a [warehouse](https://en.wikipedia.org/wiki/Data_warehouse) of analysis & ML-ready datasets (a [feature store](https://en.wikipedia.org/wiki/Feature_engineering#Feature_stores)).\n",
    "\n",
    "While this tutorial is all about basic metadata, you'll later see that `lamindb` gives you a framework for linking complex metadata related to biology and any custom schema.\n",
    "\n",
    "```{warning}\n",
    "\n",
    "This is still work-in-progress.\n",
    "\n",
    "```\n",
    "\n",
    "```{tip}\n",
    "\n",
    "This tutorial is a [Jupyter notebook](https://github.com/laminlabs/lamindb/blob/main/docs/tutorial1.ipynb).\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up an instance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Installation and sign-up](./guide.md#setup) take no time: Run `pip install lamindb` and `lamin signup <email>` on the command line."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the CLI, let's create a LaminDB instance with a directory `./mydata` for storing files and a SQLite database for managing metadata:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lamin init --storage ./mydata  # or \"s3://my-bucket\" or \"gs://my-bucket\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Think of initializing a LaminDB instance as analogous to initializing a git repository.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to import `lamindb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lamindb as ln"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track a data source"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing where a batch of data comes from helps finding & understanding it.\n",
    "\n",
    "We call the code that generated it a _transform_. The code can be a data pipeline, a notebook or an app/instrument upload.\n",
    "\n",
    "With {class}`~lamindb.Transform`, LaminDB maintains a registry of transforms and makes it easy to link data against them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're running a Jupyter notebook. Let's track it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.track()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling {func}`~lamindb.track`, the notebook is automatically linked as the source of all data that's about to be saved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} What happened under the hood?\n",
    "\n",
    "Logging informed us about\n",
    "\n",
    "1. the package versions that the notebook imports\n",
    "2. the automatic detection of notebook metadata (title, filename, version, timestamp, creator) and creation of a {class}`~lamindb.Transform` object\n",
    "3. the automatic creation of a {class}`~lamindb.Run` object (timestamp, transform, creator)\n",
    "\n",
    ":::\n",
    "\n",
    ":::{dropdown} How do I track a versioned pipeline?\n",
    "\n",
    "If you'd like to track one of your versioned pipelines as a data source:\n",
    "\n",
    "```python\n",
    "transform = ln.Transform(name=\"My pipeline\", version=\"1.2.0\")\n",
    "ln.track(transform)\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{dropdown} Why do we care about notebooks?\n",
    "\n",
    "Most people advocate for \"not using notebooks in production\" or similar. And we agree! Anything that can be a pipeline, should be a pipeline.\n",
    "\n",
    "But we also think that a lot of the downstream insight & value generated from biological data is driven by computational biologists interacting with it.\n",
    "\n",
    "And we think this is very much akin to the prose-heavy design of biological experiments documented in an ELN.\n",
    "\n",
    "A notebook that's run a single time on specific data batches is not a pipeline, it's a _document_ that produced an insight or some other form of data representation.\n",
    "\n",
    "Unfortunately, most mistakes happen when using notebooks. `ln.track()` tries to help with avoiding some.\n",
    "\n",
    "An early blog post on this is [here](https://lamin.ai/blog/2022/nbproject).\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need some dummy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put a file \"mini.csv\" into default storage\n",
    "filepath = ln.dev.datasets.file_mini_csv(in_storage_root=True)\n",
    "# put a directory \"sample_001\" into default storage\n",
    "ln.dev.datasets.dir_scrnaseq_cellranger(\"sample_001\", ln.settings.storage)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have an existing file in our default storage location: `./mydata/mini.csv`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a {class}`~lamindb.File` object from the path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ln.File(\"./mydata/mini.csv\")  # or \"s3://my-bucket/my-folder/my-file.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} What is a File object in LaminDB?\n",
    "\n",
    "It's an object to manage the file metadata, enable search & queries of the file, and different ways of accessing the file.\n",
    "\n",
    "Basic metadata is:\n",
    "\n",
    "- `id`: a universal ID (also serves as a primary key in the underlying SQL table of the instance)\n",
    "- `key`: an optional storage key, i.e., the relative path of the file in `storage`\n",
    "- `description`: an optional string description\n",
    "- `storage`: the storage location (the root, say, an S3 bucket or network location)\n",
    "- `suffix`: the file suffix\n",
    "- `size`: the file size in bytes\n",
    "- `hash`: a hash useful to check for integrity and collisions (is this file already stored?)\n",
    "- `hash_type`: the type of the hash (usually, an MD5 or SHA1 checksum)\n",
    "- `created_at`: time of creation\n",
    "- `updated_at`: time of last update\n",
    "\n",
    "Provenance-related metadata is:\n",
    "\n",
    "- `created_by`: the {class}`~lamindb.User` who created the file\n",
    "- `transform`: the {class}`~lamindb.Transform` (pipeline, notebook, instrument, app) that was run\n",
    "- `run`: the {class}`~lamindb.Run` of the transform that created the file\n",
    "\n",
    "Managing the underlying data:\n",
    "\n",
    "- `load()`: load the file to memory for formats like `.parquet`, `.zarr`, and `.h5ad`\n",
    "- `path()`: the path (cloud or local)\n",
    "- `stage()`: a local path to a cached object\n",
    "- `replace()`: replace the content of the file\n",
    "\n",
    "For a full reference, see {class}`~lamindb.File`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By saving a file object, metadata & data are saved to database & storage in a single [ACID](/faq/acid) transaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.save()  # as the file is already in the desired storage location, only metadata is written"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we called `ln.track()`, we know where the file came from. It has linked {class}`~lamindb.Transform` and {class}`~lamindb.Run` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a new file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a local file that's not yet in a registered storage location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "filepath = ln.dev.datasets.file_jpg_paradisi05().resolve()\n",
    "\n",
    "filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it's not found in a storage location, we're now getting a hint that tells us it will will be copied into default storage upon `.save()`.\n",
    "\n",
    "This behavior is useful when you're working with local caches and want to upload \"final\" data to the cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ln.File(filepath, description=\"paradisi05 laminopathic nuclei image\")\n",
    "\n",
    "# Optionally, you may specify the target path for storing the file by passing the `key` argument\n",
    "# this will store the file as `./mydata/images/paradisi05_laminopathic_nuclei.jpg`\n",
    "# file = ln.File(filepath, key=\"images/paradisi05_laminopathic_nuclei.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the default storage, we see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ln.File.tree()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the database, you'll see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ln.view()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{meth}`~lamindb.File.path` will give you the filepath:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.path()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file is in the cloud, you typically stage a cached file ({meth}`~lamindb.File.stage`) or stream its data ({meth}`~lamindb.File.backed`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search or query the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can search the file based on the fields in the `File` registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.search(\"paradisi\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can query the file by any metadata combination: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ln.User.lookup()  # auto-complete users\n",
    "transform = ln.Transform.filter(\n",
    "    name__contains=\"files & datasets\"\n",
    ").one()  # query name field of Transform registry, expect *exactly* one result\n",
    "\n",
    "ln.File.filter(\n",
    "    suffix=\".jpg\",\n",
    "    created_by=users.testuser1,\n",
    "    transform=transform,\n",
    ").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also chain `.filter()` and `.search()` statements, e.g. `ln.File.filter(suffix=\".jpg\").search(\"my image\")`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An empty filter gives you the entire registry content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.filter().df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use {meth}`~lamindb.File.from_dir` to create files from a directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ln.File.from_dir(\"./mydata/sample_001/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.save(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View as a tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.tree(\"./mydata/sample_001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or as a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.filter(key__startswith=\"sample_001/\").df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "LaminDB treats directories similar to AWS S3, as a prefix in the storage `key`, queryable with `key__startswith`.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage features & labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} Why do we care about managing features & labels?\n",
    "\n",
    "1. Finding data: Which datasets measured expression of cell marker CD14? Which datasets have a test & train split? Which characterized cell line K562? Etc.\n",
    "2. Validating data: Are there typos in feature names? Are there typos in sampled labels? Are units of features consistent? Etc.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} A perspective on contextualizing data objects\n",
    "\n",
    "We have come to love the pydata family of data objects: `DataFrame`, `AnnData`, `pytorch.DataLoader`, `zarr.Array`, `pyarrow.Table`, `xarray.Dataset`, ...\n",
    "\n",
    "But we couldnâ€™t find an object for linking data objects to context!\n",
    "\n",
    "So, we made `lamindb.File` and `lamindb.Dataset` to model how data objects relate to their context.\n",
    "\n",
    "Context can be other data objects, data transformations, ML models, users & pipelines that performed transformations (all aspects of data lineage).\n",
    "\n",
    "Context can also be any entity of the domain in which data is generated and modeled.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a batch of the Iris flower dataset in the form of a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ln.dev.datasets.df_iris_in_meter_batch1()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate & link features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use {meth}`~lamindb.File.from_df` to track this DataFrame along with its columns as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ln.File.from_df(df, description=\"Iris flower dataset batch 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is an empty LaminDB instance without a single registered feature, we are informed that features couldn't be validated and are ignored.\n",
    "\n",
    "But, all features here are meaningful and well-curated, so, let's create records for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ln.Feature.from_df(df)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as we save them, they'll serve as the reference for validating data batches that we'd like to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.save(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} How to track units of features?\n",
    "\n",
    "It's easy using {class}`~lamindb.Feature.unit`. In the above example, you'd do:\n",
    "\n",
    "```python\n",
    "for feature in features:\n",
    "    if feature.type == \"float\":\n",
    "        feature.unit = \"m\"\n",
    "        feature.save()\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now create a `File` object again, we'll see that features are validated based on the registry content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ln.File.from_df(df, description=\"Iris flower dataset batch 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's register the file along with its linked features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get an overview of all linked feature sets by `slot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `slot` provides a string key to access feature sets. It's typically the accessor of feature identifiers in the data object we're validating & registering (here, a `DataFrame`).\n",
    "\n",
    "Let's use it to access all linked features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.features[\"columns\"].df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate & link labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris dataset comes with labels within the data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_labels = ln.Label.from_values(df[\"iris_species_name\"])\n",
    "\n",
    "ln.save(species_labels)\n",
    "\n",
    "species_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also annotate the file with the labels for `iris_species_name` that are sampled in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.add_labels(species_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This enables to query & search the file by whether \"setosa\" was sampled in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.filter(labels__name=\"setosa\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or for a given feature of a file, which labels are present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.get_labels(\"iris_species_name\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to features present in columns, a file can be labeled with additional metadata:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say this file belongs to `\"experiment_1\"` and we'd like to track this information for two reasons: \n",
    "\n",
    "1. later we'd like to query all files link to this experiment\n",
    "2. we consider it a potential confounder when we'll analyze similar data from a follow-up experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment1 = ln.Label(name=\"experiment_1\")\n",
    "\n",
    "experiment1.save()\n",
    "\n",
    "experiment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Feature(name=\"experiment\", type=\"category\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.add_labels(experiment1, feature=\"experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You notice a new feature set is created for slot \"ext\" (external):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the database content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.view(registries=[\"Feature\", \"FeatureSet\", \"Label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage datasets\n",
    "\n",
    "In simple cases as we just saw, we can use files to store datasets.\n",
    "\n",
    "In more complex cases, however, we'd like store collections of images, collections of data objects, or SQL tables in BigQuery, Snowflake, or Postgres.\n",
    "\n",
    "Hence, we need a second central class for data storage: {class}`~lamindb.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ln.dev.datasets.df_iris_in_meter_batch2()\n",
    "ln.File.from_df(df, description=\"Iris flower dataset batch 2\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = ln.File.filter(description=\"Iris flower dataset batch 1\").one()\n",
    "file2 = ln.File.filter(description=\"Iris flower dataset batch 2\").one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ln.Dataset.from_files(name=\"The combined Iris dataset\", files=[file1, file2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} What is a Dataset?\n",
    "\n",
    "Basic dataset metadata is:\n",
    "\n",
    "id: a universal ID that also serves as a primary key in the SQL table\n",
    "name: a name\n",
    "hash: an MD5 hash useful to check for integrity and collisions\n",
    "file: a link to a single file, if the dataset consists in a single file\n",
    "files: a link to several files, if the dataset consists in several files (is \"sharded\")\n",
    "created_at: time of creation\n",
    "updated_at: time of last update\n",
    "created_by: the {class}~lamindb.User who created the file\n",
    "Managing the underlying data:\n",
    "\n",
    "load(): load the file to memory for formats like .parquet, .zarr, and .h5ad\n",
    "backed(): the path (cloud or local)\n",
    "For a full reference, see {class}~lamindb.Dataset.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the sharded dataset as if it was one dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the underlying two file objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.files.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or see the registries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.view(registries=[\"Dataset\", \"File\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To end this guide through basic file & metadata tracking, let's see how to update records storing metadata for any entity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate records upon creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already created a `project_1` label before, let's see what happens if we try to create it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ln.Label(name=\"project_1\")\n",
    "\n",
    "label.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of creating a new record, LaminDB will load and return the existing record from the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is no exact match, LaminDB will warn you upon creating a record about potential duplictes.\n",
    "\n",
    "Say, we spell \"project_1\" without an underscore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Label(name=\"project 1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that for every record creation, a search compares whether a similar already exists!\n",
    "    \n",
    "This is to avoid inserting duplicated records.\n",
    "\n",
    "You can switch it off (for performance gains) via `ln.settings.upon_create_search_names = False`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ln.Label.filter(name=\"project_1\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.name = \"project_1a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete records like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default storage location is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.settings.storage  # your \"working data directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change it by setting `ln.settings.storage = \"s3://my-bucket\"` and see all storage locations via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Storage.filter().df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# clean up what we wrote in this notebook\n",
    "!lamin delete mydata\n",
    "!rm -r mydata\n",
    "!rm paradisi05_laminopathic_nuclei.jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "nbproject": {
   "id": "NJvdsWWbJlZS",
   "parent": null,
   "pypackage": null,
   "time_init": "2023-03-09T13:30:29.239953+00:00",
   "user_handle": "testuser2",
   "user_id": "bKeW4T6E",
   "user_name": "Test User2",
   "version": "0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae1fefc8646a06dd2e75004cd934adda7c5727b046986a772e3b44b0ffba9754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
