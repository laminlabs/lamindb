{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage files & datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn messy files into validated, queryable datasets.\n",
    "\n",
    "```{note}\n",
    "\n",
    "- This tutorial as [Jupyter notebook](https://github.com/laminlabs/lamindb/blob/main/docs/tutorial1.ipynb).\n",
    "- This tutorial manages metadata schema-less, but `lamindb` gives you a framework for managing complex typed metadata schema-full.\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up an instance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Installation and sign-up](./guide.md#setup) take no time: Run `pip install lamindb` and `lamin signup <email>` on the command line."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the CLI, create a LaminDB instance with a directory `./mydata` for storing data and a SQLite database for managing it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!lamin init --storage ./mydata  # or \"s3://my-bucket\" or \"gs://my-bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} What else can I configure during setup?\n",
    "\n",
    "1. Instead of SQLite, use PostgreSQL: `--db postgresql://<user>:<pwd>@<hostname>:<port>/<dbname>`\n",
    "2. Beyond the core schema, use bionty and other schemas: `--schema bionty,custom1,template1`\n",
    "3. Instead of an instance name that's derived from default storage, provide a custom name: `--name myinstance`\n",
    "\n",
    "For more, see {doc}`./guide/setup`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track a data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lamindb as ln"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing where a batch of data comes from helps finding & understanding it.\n",
    "\n",
    "Let's call the code that generated it a transform. This code can be a data pipeline, a notebook or an app upload.\n",
    "\n",
    "With {class}`~lamindb.Transform`, LaminDB maintains a registry of transforms and makes it easy to link data against them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're running a Jupyter notebook. Let's track it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.track()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling {func}`~lamindb.track`, the notebook is automatically linked as the source of all data that's about to be saved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} What happened under the hood?\n",
    "\n",
    "1. Imported package versions were detected\n",
    "2. Notebook metadata was detected and stored in a {class}`~lamindb.Transform` record (title, filename, version, timestamp, creator)\n",
    "3. A run {class}`~lamindb.Run` record was created (timestamp, transform, creator)\n",
    "\n",
    ":::\n",
    "\n",
    ":::{dropdown} How do I track a pipeline instead of a notebook?\n",
    "\n",
    "```python\n",
    "transform = ln.Transform(name=\"My pipeline\", version=\"1.2.0\")\n",
    "ln.track(transform)\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{dropdown} Why do we care about tracking notebooks?\n",
    "\n",
    "Most people advocate for \"not using notebooks in production\" or similar. And we agree! Anything that can be a pipeline, should be a pipeline.\n",
    "\n",
    "But we also think that a lot of the downstream insight & value generated from biological data is driven by computational biologists interacting with it.\n",
    "\n",
    "And we think this is very much akin to the prose-heavy design of biological experiments documented in an ELN.\n",
    "\n",
    "A notebook that's run a single time on specific data batches is not a pipeline, it's a _document_ that produced an insight or some other form of data representation.\n",
    "\n",
    "Unfortunately, most mistakes happen when using notebooks. `ln.track()` tries to help with avoiding some.\n",
    "\n",
    "An early blog post on this is [here](https://lamin.ai/blog/2022/nbproject).\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need some dummy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a file \"./mydata/mini.csv\" in default storage\n",
    "ln.dev.datasets.file_mini_csv(in_storage_root=True)\n",
    "# a directory \"./mydata/sample_001\" in default storage\n",
    "ln.dev.datasets.dir_scrnaseq_cellranger(\"sample_001\", ln.settings.storage)\n",
    "# a file \"paradisi05_laminopathic_nuclei.jpg\" in the working directory\n",
    "ln.dev.datasets.file_jpg_paradisi05().resolve()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There already is a file in default storage: `./mydata/mini.csv`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a {class}`~lamindb.File` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ln.File(\"./mydata/mini.csv\")  # or \"s3://my-bucket/my-folder/my-file.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} What is the File class?\n",
    "\n",
    "It manages file metadata to enable search & queries of the file, validation of file content, and different ways of access.\n",
    "\n",
    "Basic fields are:\n",
    "\n",
    "- `id`: a universal ID (also serves as a primary key in the underlying SQL table of the instance)\n",
    "- `key`: an optional storage key, i.e., the relative path of the file in `storage`\n",
    "- `description`: an optional string description\n",
    "- `storage`: the storage location (the root, say, an S3 bucket or network location)\n",
    "- `suffix`: the file suffix\n",
    "- `size`: the file size in bytes\n",
    "- `hash`: a hash useful to check for integrity and collisions (is this file already stored?)\n",
    "- `hash_type`: the type of the hash (usually, an MD5 or SHA1 checksum)\n",
    "- `created_at`: time of creation\n",
    "- `updated_at`: time of last update\n",
    "\n",
    "Provenance-related fields are:\n",
    "\n",
    "- `created_by`: the {class}`~lamindb.User` who created the file\n",
    "- `transform`: the {class}`~lamindb.Transform` (pipeline, notebook, instrument, app) that was run\n",
    "- `run`: the {class}`~lamindb.Run` of the transform that created the file\n",
    "\n",
    "Access the path through a property:\n",
    "\n",
    "- `path`: the file path\n",
    "\n",
    "Access the underlying data:\n",
    "\n",
    "- `load()`: load the file to memory for formats like `.parquet`, `.zarr`, and `.h5ad`\n",
    "- `backed()`: stream/query the file from the cloud\n",
    "- `stage()`: a local path to a cached object\n",
    "- `replace()`: replace the content of the file\n",
    "\n",
    "For a full reference, see {class}`~lamindb.File`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By saving the file, metadata & data are saved to database & storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.save()  # as the file is already in a registered storage location, only metadata is written"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we called {func}`~lamindb.track`, we know where the file came from ({class}`~lamindb.Transform` & {class}`~lamindb.Run`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a new file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a local file that's not yet in a registered storage location: `./paradisi05_laminopathic_nuclei.jpg`\n",
    "\n",
    "Upon `.save()` it will be copied (or uploaded) to default storage (here, `./mydata`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ln.File(\n",
    "    \"paradisi05_laminopathic_nuclei.jpg\",\n",
    "    description=\"paradisi05 laminopathic nuclei image\",\n",
    ")  # or with key=\"images/paradisi05_laminopathic_nuclei.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In storage are two files. One with a human-readable key (`'mini.csv'`) and one that's auto-keyed by LaminDB with its file id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ln.File.tree()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database stores context for both files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.view()  # link tables in the database are not shown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{attr}`~lamindb.File.path` gives you a filepath:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.path  # or \"s3://my-bucket/my-folder/my-file.jpg\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file is in the cloud, you typically stage a cached file ({meth}`~lamindb.File.stage`) or stream its data ({meth}`~lamindb.File.backed`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search or query the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can search the file based on the fields in the {class}`~lamindb.File` registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.search(\"paradisi\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, query the file by any metadata combination: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ln.User.lookup()  # look up users with auto-complete\n",
    "transform = ln.Transform.filter(\n",
    "    name__contains=\"files & datasets\"\n",
    ").one()  # query transforms, expect *exactly* one result\n",
    "\n",
    "ln.File.filter(\n",
    "    suffix=\".jpg\",\n",
    "    created_by=users.testuser1,\n",
    "    transform=transform,\n",
    ").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also chain `.filter()` and `.search()` statements, e.g. `ln.File.filter(suffix=\".jpg\").search(\"my image\")`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An empty filter returns the entire registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.filter().df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more info, see: {doc}`guide/select`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.view_lineage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a comprehensive example with handovers from app uploads, pipelines & notebooks of multiple data types, see {doc}`guide/data-lineage`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage features & labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} Why care about features & labels?\n",
    "\n",
    "1. Finding data: Which datasets measured expression of cell marker CD14? Which characterized cell line K562? Which datasets have a test & train split? Etc.\n",
    "2. Validating data: Are there typos in feature names? Are there typos in sampled labels? Are units of features consistent? Etc.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} A perspective on contextualizing data objects\n",
    "\n",
    "We love the pydata family of data objects: `DataFrame`, `AnnData`, `pytorch.DataLoader`, `zarr.Array`, `pyarrow.Table`, `xarray.Dataset`, ...\n",
    "\n",
    "But we couldn’t find an object for linking data objects to context!\n",
    "\n",
    "So, we made `lamindb.File` and `lamindb.Dataset` to model how data objects relate to their context:\n",
    "\n",
    "- other data objects, data transformations, models, users & pipelines that performed transformations (provenance)\n",
    "- any entity of the domain in which data is generated and modeled (biology)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a batch of the Iris flower dataset (a `DataFrame`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ln.dev.datasets.df_iris_in_meter_batch1()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate & link features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use {meth}`~lamindb.File.from_df` to track this DataFrame along with its columns as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ln.File.from_df(df, description=\"Iris flower dataset batch 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features couldn't be validated and are ignored because this is an empty LaminDB instance without a single registered feature.\n",
    "\n",
    "But, all features here are meaningful and well-curated, so, let's create records for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "features = ln.Feature.from_df(df)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as we save them, they'll serve as the reference for validating data batches that we'd like to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.save(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} How to track units of features?\n",
    "\n",
    "It's easy using {class}`~lamindb.Feature.unit`. In the above example, you'd do:\n",
    "\n",
    "```python\n",
    "for feature in features:\n",
    "    if feature.type == \"float\":\n",
    "        feature.unit = \"m\"  # SI unit for meters\n",
    "        feature.save()\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create the `File` now, we'll see that features are validated based on the registry content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ln.File.from_df(df, description=\"Iris flower dataset batch 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's register the file along with its linked features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an overview of linked feature sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `slot` provides a string key to access feature sets. It's typically the accessor of feature identifiers in the data object we're validating & registering (here, a `DataFrame`).\n",
    "\n",
    "Let's use it to access all linked features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.features[\"columns\"].df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate & link labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris dataset comes with labels within the data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "species_labels = ln.Label.from_values(df[\"iris_species_name\"])\n",
    "\n",
    "species_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save them to the {class}`~lamindb.Label` registry so that they get validated going forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.save(species_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And annotate the file with the labels for feature `iris_species_name`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.add_labels(species_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get linked labels from a feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.get_labels(\"iris_species_name\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now query & search the file by whether `\"setosa\"` is linked to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.filter(labels__name=\"setosa\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to features present _within_ a data object like a `DataFrame`, a file can be labeled with external metadata.\n",
    "\n",
    "Let's label this file with `\"experiment_1\"`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment1 = ln.Label(name=\"experiment_1\")\n",
    "experiment1.save()\n",
    "experiment1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} Why labeling a data batch by experiment?\n",
    "\n",
    "We can then\n",
    "\n",
    "1. query all files link to this experiment\n",
    "2. model it as a confounder when we'll analyze similar data from a follow-up experiment, and concatenate data using the label as a feature in a data matrix\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also register a feature that holds experiment labels in concatenated datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Feature(name=\"experiment\", type=\"category\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.add_labels(experiment1, feature=\"experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the original feature set and the external dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the context for our file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the database content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.view(registries=[\"Feature\", \"FeatureSet\", \"Label\", \"Modality\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage datasets\n",
    "\n",
    "In simple cases as we just saw, we can use files to store datasets.\n",
    "\n",
    "In more complex cases, we'd like store collections of files, data in mutable storage backends (zarr, TileDB, DuckDB, etc.) or in SQL tables in BigQuery, Snowflake, or Postgres.\n",
    "\n",
    "Hence, we need a second central class for data storage: {class}`~lamindb.Dataset`.\n",
    "\n",
    "Let's say we have a second batch of the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "df = ln.dev.datasets.df_iris_in_meter_batch2()\n",
    "ln.File.from_df(df, description=\"Iris flower dataset batch 2\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And load both files storing these batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = ln.File.filter(description=\"Iris flower dataset batch 1\").one()\n",
    "file2 = ln.File.filter(description=\"Iris flower dataset batch 2\").one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a sharded dataset from these two batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ln.Dataset.from_files(name=\"The combined Iris dataset\", files=[file1, file2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the sharded dataset as if it was one dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.load().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the underlying two file objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.files.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.files.list()[0].view_lineage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or see the registries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.view(registries=[\"Dataset\", \"File\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more interesting data lineage graph, let's pretend we're now running a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "pipeline = ln.Transform(name=\"Iris Postprocessor\", version=\"0.7.2\")\n",
    "ln.track(pipeline)  # create & track a pipeline\n",
    "input_files = ln.File.filter(transform__name__contains=\"files & datasets\").all()\n",
    "[file.stage() for file in input_files]  # let's load the input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use {meth}`~lamindb.File.from_dir` to create files from a directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ln.File.from_dir(\"./mydata/sample_001/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.save(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View as a tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "ln.File.tree(\"./mydata/sample_001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or as a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.filter(key__startswith=\"sample_001/\").df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "LaminDB treats directories similar to AWS S3, as a prefix in the storage `key`, queryable with `key__startswith`.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a summary for all files ingested from this notebook (interactive exploration will be possible in the app):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[0].view_lineage()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To end this guide through basic file & metadata tracking, let's see how to update registry records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata ontologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say, we want to express that `experiment_1` belongs to project 1. This is how we can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project1 = ln.Label(name=\"project_1\")\n",
    "project1.save()\n",
    "experiment1.parents.add(project1)\n",
    "experiment1.view_parents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more info, see {meth}`~lamindb.dev.Registry.view_parents`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate records upon creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already created a `project_1` label before, let's see what happens if we try to create it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ln.Label(name=\"project_1\")\n",
    "\n",
    "label.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of creating a new record, LaminDB will load and return the existing record from the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is no exact match, LaminDB will warn you upon creating a record about potential duplictes.\n",
    "\n",
    "Say, we spell \"project_1\" without an underscore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Label(name=\"project 1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that for every record creation, a search compares whether a similar already exists!\n",
    "    \n",
    "This is to avoid inserting duplicated records.\n",
    "\n",
    "You can switch it off (for performance gains) via `ln.settings.upon_create_search_names = False`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ln.Label.filter(name=\"project_1\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.name = \"project_1a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete records like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default storage location is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.settings.storage  # your \"working data directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change it by setting `ln.settings.storage = \"s3://my-bucket\"` and see all storage locations via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Storage.filter().df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.settings.verbosity = 3  # only show info, no hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# clean up what we wrote in this notebook\n",
    "!lamin delete mydata\n",
    "!rm -r mydata\n",
    "!rm paradisi05_laminopathic_nuclei.jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "nbproject": {
   "id": "NJvdsWWbJlZS",
   "parent": null,
   "pypackage": null,
   "time_init": "2023-03-09T13:30:29.239953+00:00",
   "user_handle": "testuser2",
   "user_id": "bKeW4T6E",
   "user_name": "Test User2",
   "version": "0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae1fefc8646a06dd2e75004cd934adda7c5727b046986a772e3b44b0ffba9754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
