{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files & datasets, pipelines & analyses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first tutorial you'll learn LaminDB's basic data management workflow.\n",
    "\n",
    "You'll start with a [data lake](https://en.wikipedia.org/wiki/Data_lake) of files and arrive at a [warehouse](https://en.wikipedia.org/wiki/Data_warehouse) of analysis & ML-ready datasets (a [feature store](https://en.wikipedia.org/wiki/Feature_engineering#Feature_stores)).\n",
    "\n",
    "All accessible through one API & App!\n",
    "\n",
    "```{tip}\n",
    "\n",
    "You can run this tutorial as a [Jupyter notebook](https://github.com/laminlabs/lamindb/blob/main/docs/guide/01-tutorial1.ipynb).\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a LaminDB instance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Installing LaminDB and signing up](./index#setup) takes 2 min. ‚úÖ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the CLI, let's create a LaminDB instance with a directory `./mydata` for storing files and a SQLite database for managing metadata:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lamin init --storage ./mydata  # or \"s3://my-bucket\" or \"gs://my-bucket\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Think of initializing a LaminDB instance as analogous to initializing a git repository.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to import `lamindb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lamindb as ln\n",
    "\n",
    "ln.settings.verbosity = 3  # show hints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default storage location is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.settings.storage  # your \"working data directory\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change it by setting `ln.settings.storage = \"s3://my-bucket\"`.\n",
    "\n",
    "And you can see all storage locations by querying {class}`~lamindb.Storage`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Storage.select().df()  # SQLite only supports a single location, Postgres supports an arbitrary number"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll walk through managing:\n",
    "\n",
    "1. _files_ (as in a data lake)\n",
    "2. _datasets_ in form of collections of files, _data objects_ (`DataFrame`, `AnnData`) or SQL tables (as in a feature store or data warehouse)\n",
    "3. _metadata_ in form of _transforms_ (notebooks & pipelines), _runs_, _features_, _tags_, _projects_ & _users_\n",
    "\n",
    "In later material, you'll see that `lamindb` gives you a full framework for linking metadata related to [data lineage](./data-lineage), [biology](./registries) and any [custom schema](https://github.com/laminlabs/lnschema-lamin1)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track a data source"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing where a piece of data comes from greatly helps with finding & understanding it. üëç\n",
    "\n",
    "The data source is the code that generates the data, which we model as a \"transform\", a {class}`~lamindb.Transform` object.\n",
    "\n",
    "A transform can be a data pipeline, a notebook or an app or instrument upload.\n",
    "\n",
    "With {class}`~lamindb.Transform`, LaminDB maintains a registry of your transforms and makes it easy to link data against them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're running a Jupyter notebook. Let's track it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.track()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling {func}`~lamindb.track`, the notebook will **automatically** be linked as the source of all data that's about to be saved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens under the hood? Logging informed us about\n",
    "\n",
    "1. the package versions that the notebook imports\n",
    "2. the automatic detection of notebook metadata (id, title, filename, version, timestamp, creator) and creation of a {class}`~lamindb.Transform` object with id `NJvdsWWbJlZSry`\n",
    "3. the automatic creation of a {class}`~lamindb.Run` object (id, timestamp, transform, creator)\n",
    "\n",
    ":::{note}\n",
    "\n",
    "If you'd like to track one of your versioned pipelines as a data source:\n",
    "\n",
    "```{python}\n",
    "transform = ln.Transform(name=\"My pipeline\", version=\"1.2.0\")\n",
    "ln.track(transform)\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track an existing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# put a file \"mini.csv\" into our default storage\n",
    "filepath = ln.dev.datasets.file_mini_csv()\n",
    "filepath.rename(ln.setup.settings.storage.root / filepath.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an existing file in our storage location: `./mydata/mini.csv`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a {class}`~lamindb.File` object from the path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ln.File(\"./mydata/mini.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} File overview\n",
    "\n",
    "Basic file metadata is:\n",
    "\n",
    "- `id`: a universal ID that also serves as a primary key in the SQL table\n",
    "- `key`: the storage key, i.e., the relative path of the file in the storage location\n",
    "- `storage`: the storage location (the root, say, an S3 bucket)\n",
    "- `suffix`: the file suffix\n",
    "- `size`: the file size in bytes\n",
    "- `hash`: an MD5 checksum useful to check for integrity and collisions (is this file already stored?)\n",
    "- `created_at`: time of creation\n",
    "- `updated_at`: time of last update\n",
    "\n",
    "Provenance-related metadata is:\n",
    "\n",
    "- `created_by`: the {class}`~lamindb.User` who created the file\n",
    "- `transform`: the {class}`~lamindb.Transform` (pipeline, notebook, instrument, app) that was run\n",
    "- `run`: the {class}`~lamindb.Run` of the transform that created the file\n",
    "\n",
    "Managing the underlying data:\n",
    "\n",
    "- `load()`: load the file to memory for formats like `.parquet`, `.zarr`, and `.h5ad`\n",
    "- `path()`: the path (cloud or local)\n",
    "- `stage()`: a local path to a cached object\n",
    "- `replace()`: replace the content of the file\n",
    "\n",
    "For a full reference, see {class}`~lamindb.File`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By saving a file object, metadata & data are saved to database & storage in a single [ACID](/faq/acid) transaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.save()  # as the file is already in the desired storage location, only metadata is written"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file now has linked transform and run objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "filepath = ln.dev.datasets.file_jpg_paradisi05().resolve()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a local file that's not yet in LaminDB storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way you indicate the target path for storing the file is by passing the `key` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ln.File(filepath, key=\"images/paradisi05_laminopathic_nuclei.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into our default storage, we see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ln.File.tree()  # this also shows the SQLite database `mydata.lndb` holding metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see your files also in the SQL database together with entries for storage and users (and later down this guide, many other entities):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ln.view()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access a file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{meth}`~lamindb.File.stage` will give you a filepath to a local file, also for a cloud-based file (it will cache a cloud object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.stage()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the full `path` within the storage location (say, in an S3 bucket), we use {meth}`~lamindb.File.path`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query or search a file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can query the file by its metadata. The simplest way is by `key`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ln.File.select(key=\"images/paradisi05_laminopathic_nuclei.jpg\").one()\n",
    "\n",
    "file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can search the file by its metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.search(\"paradisi\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-memory data objects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `File` object can also be created from an in-memory data object like a `DataFrame` or an `AnnData`.\n",
    "\n",
    "For this, you'd call one of:\n",
    "\n",
    "- `file = ln.File(df, name=\"My dataset X\")`\n",
    "- `file = ln.File(df, key=\"my_folder/my_file.parquet\")`\n",
    "- `file = ln.File.from_df(df, name=\"My dataset X\")  # will track column names as features`\n",
    "- `file = ln.File.from_df(df, key=\"my_folder/my_file.parquet\")  # will track column names as features`\n",
    "\n",
    "\n",
    "Under-the-hood, the object will be serialized into a configurable storage format (e.g. `DataFrame` ‚Üí `.parquet`, `AnnData` ‚Üí `.h5ad`/`.zrad`, ...).\n",
    "\n",
    "However, while this is the way to go for \"auxialiary datasets\", if your dataset is relevant for a later analysis, you'd rather create a `Dataset` instead of a `File` in such cases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A single DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the simplest case in which a dataset corresponds to a single `DataFrame`, which we'll store as a `File` object (a `.parquet` file in storage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ln.dev.datasets.df_iris_in_meter_batch1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ln.Dataset(df, name=\"Iris flower dataset batch1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataframe back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.load().head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` object has a 1:1 correspondence to an underlying file object, accessible via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you can stage the underlying parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.file.stage()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data got added with a storage key based on the `id`, because here, we didn't pass the `key` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ln.File.tree()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the database, we're now seeing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.view()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} Dataset overview\n",
    "\n",
    "Basic dataset metadata is:\n",
    "\n",
    "- `id`: a universal ID that also serves as a primary key in the SQL table\n",
    "- `name`: a name\n",
    "- `hash`: an MD5 hash useful to check for integrity and collisions\n",
    "- `file`: a link to a single file, if the dataset consists in a single file\n",
    "- `files`: a link to several files, if the dataset consists in several files (is \"sharded\")\n",
    "- `created_at`: time of creation\n",
    "- `updated_at`: time of last update\n",
    "- `created_by`: the {class}`~lamindb.User` who created the file\n",
    "\n",
    "Managing the underlying data:\n",
    "\n",
    "- `load()`: load the file to memory for formats like `.parquet`, `.zarr`, and `.h5ad`\n",
    "- `backed()`: the path (cloud or local)\n",
    "\n",
    "For a full reference, see {class}`~lamindb.Dataset`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple DataFrames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we measure data in batches and want to store these batches separately.\n",
    "\n",
    "Let us look at how to construct a `Dataset` from two (or more) files corresponding to these batches (or \"shards\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = dataset.file\n",
    "file2 = ln.File.from_df(\n",
    "    ln.dev.datasets.df_iris_in_meter_batch2(), description=\"Iris batch 2\"\n",
    ")\n",
    "file2.save()  # we have to save a file before using it to compose a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ln.Dataset.from_files(name=\"The combined Iris dataset\", files=[file1, file2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the sharded dataset as if it was one dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In storage, you see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.tree()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the database, you see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.view()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand data objects in context \n",
    "\n",
    "We have come to love the pydata family of data objects: `DataFrame`, `AnnData`, `pytorch.DataLoader`, `zarr.Array`, `pyarrow.Table`, `xarray.Dataset`, and others.\n",
    "\n",
    "But we couldn‚Äôt find an object for linking data objects to context! üò†\n",
    "\n",
    "So, we made `lamindb.File` and `lamindb.Dataset` to model how data objects relate to their context.\n",
    "\n",
    "Context can be other data objects, data transformations, ML models, users & pipelines that performed transformations (all aspects of data lineage).\n",
    "\n",
    "Context can also be any entity of the domain in which data is generated and modeled.\n",
    "\n",
    "We focused on linking `File` and `Dataset` to data lineage & biological concepts. You'll learn about them further down the guide."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# generate some files in default storage\n",
    "ln.dev.datasets.generate_cell_ranger_files(\"sample_001\", ln.settings.storage)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass an existing directory to {meth}`~lamindb.File.from_dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ln.File.from_dir(\"./mydata/sample_001/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.save(files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the files as a tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.tree()  # to subset, call ln.File.tree(\"sample_001\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under-the-hood, the following records got written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.select(key__startswith=\"sample_001/\").df().head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query a specific file by passing the full key to `ln.select`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.select(key=\"sample_001/metrics_summary.csv\").df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that LaminDB treats directories similar to S3, as a plain prefix in the storage `key`.\n",
    "\n",
    "If you want to flexibly group files, consider tags ({class}`~lamindb.Tag`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say, we want to tag the files related to `sample_0001` independent of where they are in storage.\n",
    "\n",
    "Let's create and save a tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = ln.Tag(name=\"Sample 0001\")\n",
    "tag.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now label each file in `files` with this tag and save the update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    file.tags.add(tag)\n",
    "ln.save(files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now query by this tag (and arbitrarily more):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.select(tags=tag).df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, update & delete validated metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To end this guide through basic file & metadata tracking, let's see how to update records storing metadata for any entity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create & save records"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = ln.Project(name=\"Project A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = [ln.Project(name=name) for name in [\"Project B\", \"Project C\", \"Project D\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that for every record creation, a search compares whether a similar already exists!\n",
    "    \n",
    "This is to avoid inserting duplicated records.\n",
    "\n",
    "You can switch it off (for performance gains) via `ln.settings.upon_create_search_names = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.save(projects)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if you try to create the same record again, it will load instead of re-creating it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Project(name=\"Project A\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll learn about more advanced data validation in further guides."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = ln.Project.select(name=\"Project A\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.name = \"Project 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = ln.Project.select(name=\"Project B\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# clean up what we wrote in this notebook\n",
    "!lamin delete mydata\n",
    "!rm -r mydata\n",
    "!rm paradisi05_laminopathic_nuclei.jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "nbproject": {
   "id": "NJvdsWWbJlZS",
   "parent": null,
   "pypackage": null,
   "time_init": "2023-03-09T13:30:29.239953+00:00",
   "user_handle": "testuser2",
   "user_id": "bKeW4T6E",
   "user_name": "Test User2",
   "version": "0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae1fefc8646a06dd2e75004cd934adda7c5727b046986a772e3b44b0ffba9754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
