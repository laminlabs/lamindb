{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage files & datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you'll learn LaminDB's basic data management workflow.\n",
    "\n",
    "Starting with a [lake](https://en.wikipedia.org/wiki/Data_lake) of files, you'll arrive at a [warehouse](https://en.wikipedia.org/wiki/Data_warehouse) of analysis & ML-ready datasets (a [feature store](https://en.wikipedia.org/wiki/Feature_engineering#Feature_stores)).\n",
    "\n",
    "While this tutorial is all about basic metadata, you'll later see that `lamindb` gives you a framework for linking complex metadata related to biology and any custom schema.\n",
    "\n",
    "```{tip}\n",
    "\n",
    "This tutorial is a [Jupyter notebook](https://github.com/laminlabs/lamindb/blob/main/docs/guide/tutorial1.ipynb).\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up an instance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Installation and sign-up](./index.md#setup) take no time: Run `pip install lamindb` and `lamin signup <email>` on the command line."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the CLI, let's create a LaminDB instance with a directory `./mydata` for storing files and a SQLite database for managing metadata:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lamin init --storage ./mydata  # or \"s3://my-bucket\" or \"gs://my-bucket\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Think of initializing a LaminDB instance as analogous to initializing a git repository.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to import `lamindb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lamindb as ln\n",
    "\n",
    "ln.settings.verbosity = 3  # show hints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default storage location is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.settings.storage  # your \"working data directory\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change it by setting `ln.settings.storage = \"s3://my-bucket\"`.\n",
    "\n",
    "And you can see all storage locations by querying {class}`~lamindb.Storage`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Storage.select().df()  # more on select statements later!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track a data source"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing where a batch of data comes from helps finding & understanding it.\n",
    "\n",
    "We call the code that generated it a _transform_. The code can be a data pipeline, a notebook or an app/instrument upload.\n",
    "\n",
    "With {class}`~lamindb.Transform`, LaminDB maintains a registry of transforms and makes it easy to link data against them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're running a Jupyter notebook. Let's track it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.track()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling {func}`~lamindb.track`, the notebook is automatically linked as the source of all data that's about to be saved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} What happened under the hood?\n",
    "\n",
    "Logging informed us about\n",
    "\n",
    "1. the package versions that the notebook imports\n",
    "2. the automatic detection of notebook metadata (title, filename, version, timestamp, creator) and creation of a {class}`~lamindb.Transform` object\n",
    "3. the automatic creation of a {class}`~lamindb.Run` object (timestamp, transform, creator)\n",
    "\n",
    ":::\n",
    "\n",
    ":::{dropdown} How do I track a versioned pipeline?\n",
    "\n",
    "If you'd like to track one of your versioned pipelines as a data source:\n",
    "\n",
    "```python\n",
    "transform = ln.Transform(name=\"My pipeline\", version=\"1.2.0\")\n",
    "ln.track(transform)\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{dropdown} Why do we care about notebooks?\n",
    "\n",
    "Most people advocate for \"not using notebooks in production\" or similar. And we agree! Anything that can be a pipeline, should be a pipeline.\n",
    "\n",
    "But we also think that a lot of the downstream insight & value generated from biological data is driven by computational biologists interacting with it.\n",
    "\n",
    "And we think this is very much akin to the prose-heavy design of biological experiments documented in an ELN.\n",
    "\n",
    "A notebook that's run a single time on specific data batches is not a pipeline, it's a _document_ that produced an insight or some other form of data representation.\n",
    "\n",
    "Unfortunately, most mistakes happen when using notebooks. `ln.track()` tries to help with avoiding some.\n",
    "\n",
    "An early blog post on this is [here](https://lamin.ai/blog/2022/nbproject).\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track an existing file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have an existing file in our storage location: `./mydata/mini.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# put a file \"mini.csv\" into our default storage\n",
    "filepath = ln.dev.datasets.file_mini_csv()\n",
    "filepath.rename(ln.setup.settings.storage.root / filepath.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a {class}`~lamindb.File` object from the path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ln.File(\"./mydata/mini.csv\")  # or \"s3://my-bucket/my-folder/my-file.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} What is a File object in LaminDB?\n",
    "\n",
    "It's an object to manage the file metadata, enable search & queries of the file, and different ways of accessing the file.\n",
    "\n",
    "Basic metadata is:\n",
    "\n",
    "- `id`: a universal ID (also serves as a primary key in the underlying SQL table of the instance)\n",
    "- `key`: an optional storage key, i.e., the relative path of the file in `storage`\n",
    "- `description`: an optional string description\n",
    "- `storage`: the storage location (the root, say, an S3 bucket or network location)\n",
    "- `suffix`: the file suffix\n",
    "- `size`: the file size in bytes\n",
    "- `hash`: a hash useful to check for integrity and collisions (is this file already stored?)\n",
    "- `hash_type`: the type of the hash (usually, an MD5 or SHA1 checksum)\n",
    "- `created_at`: time of creation\n",
    "- `updated_at`: time of last update\n",
    "\n",
    "Provenance-related metadata is:\n",
    "\n",
    "- `created_by`: the {class}`~lamindb.User` who created the file\n",
    "- `transform`: the {class}`~lamindb.Transform` (pipeline, notebook, instrument, app) that was run\n",
    "- `run`: the {class}`~lamindb.Run` of the transform that created the file\n",
    "\n",
    "Managing the underlying data:\n",
    "\n",
    "- `load()`: load the file to memory for formats like `.parquet`, `.zarr`, and `.h5ad`\n",
    "- `path()`: the path (cloud or local)\n",
    "- `stage()`: a local path to a cached object\n",
    "- `replace()`: replace the content of the file\n",
    "\n",
    "For a full reference, see {class}`~lamindb.File`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By saving a file object, metadata & data are saved to database & storage in a single [ACID](/faq/acid) transaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.save()  # as the file is already in the desired storage location, only metadata is written"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we called `ln.track()`, we know where the file came from. It has linked {class}`~lamindb.Transform` and {class}`~lamindb.Run` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a new file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a local file that's not yet in LaminDB storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "filepath = ln.dev.datasets.file_jpg_paradisi05().resolve()\n",
    "\n",
    "filepath"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way you indicate the target path for storing the file is by passing the `key` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ln.File(filepath, key=\"images/paradisi05_laminopathic_nuclei.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into our default storage, we see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ln.File.tree()  # this also shows the LaminDB-managed SQLite database `mydata.lndb`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the database, you'll see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ln.view()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{meth}`~lamindb.File.path` will give you a filepath within the storage location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.path()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file is in the cloud, you typically stage a cached file ({meth}`~lamindb.File.stage`) or stream its data ({meth}`~lamindb.File.backed`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search or query the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can search the file by its metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.search(\"paradisi\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can query the file by any metadata combination: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.select(key=\"images/paradisi05_laminopathic_nuclei.jpg\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ln.User.lookup(\"handle\")\n",
    "ln.File.select(created_by=users.testuser1).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = ln.Transform.select(id=\"NJvdsWWbJlZSz8\").one()\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.select(transform=transform).df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage  in-memory data objects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `File` object can also be created from an in-memory data object like a `DataFrame`, e.g.,\n",
    "\n",
    "```\n",
    "file = ln.File(df, description=\"Data batch X\")  # serialize df to storage, default is `.parquet`\n",
    "file = ln.File.from_df(df, description=\"Data batch X\")  # additionally track columns as features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a batch of the Iris flower dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "df = ln.dev.datasets.df_iris_in_meter_batch1()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use {meth}`~lamindb.File.from_df` to track this DataFrame along with its columns as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ln.File.from_df(df, description=\"Iris flower dataset batch 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query by features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the file is linked to the features it measured, we can query or search for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Feature.search(\"iris_species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_species_name = ln.Feature.select(name=\"iris_species_name\").one()\n",
    "feature_set = ln.FeatureSet.select(features=iris_species_name).one()\n",
    "ln.File.select(feature_sets=feature_set).df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate & query by labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's register the species labels and annotate the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_species = ln.Label.from_values(\n",
    "    df[\"iris_species_name\"]\n",
    ")  # create records for the sampled labels within the dataframe column\n",
    "ln.save(iris_species)  # save labels to database\n",
    "file.labels.set(iris_species)  # annotate the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This enables to query & search the file by whether \"setosa\" was sampled in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa = ln.Label.select(name=\"setosa\").one()\n",
    "ln.File.select(labels=setosa).df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, for a any given file, to see which labels were sampled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.labels.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `ref_id`, `ref_orm`, and `ref_schema` fields, we'll be able to derive labels from other registries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand data objects in context \n",
    "\n",
    "We have come to love the pydata family of data objects: `DataFrame`, `AnnData`, `pytorch.DataLoader`, `zarr.Array`, `pyarrow.Table`, `xarray.Dataset`, and others.\n",
    "\n",
    "But we couldnâ€™t find an object for linking data objects to context! ðŸ˜ \n",
    "\n",
    "So, we made `lamindb.File` and `lamindb.Dataset` to model how data objects relate to their context.\n",
    "\n",
    "Context can be other data objects, data transformations, ML models, users & pipelines that performed transformations (all aspects of data lineage).\n",
    "\n",
    "Context can also be any entity of the domain in which data is generated and modeled.\n",
    "\n",
    "We focused on linking `File` and `Dataset` to data lineage & biological concepts. You'll learn about them further down the guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we just saw, by using {meth}`~lamindb.File.from_df`, `lamindb` automatically linked features and warned us about the creation of new {class}`~lamindb.Feature` and {class}`~lamindb.Label` records.\n",
    "\n",
    "We see the result in the database overview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.view(orms=[\"Label\", \"Feature\", \"FeatureSet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} Why do we care about managing features?\n",
    "\n",
    "1. Finding a dataset: Which datasets measured expression of cell marker CD14? Which datasets have an out-of-domain split?\n",
    "2. Validating integrity: Are there typos in feature names? Are there typos in sampled labels? Are units of features consistent?\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see an evident place where we might want to further populate `Feature` records: the unit of measure for float-typed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_in_meters = ln.Feature.select(type=\"float\").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features_in_meters:\n",
    "    feature.unit = \"m\"\n",
    "    feature.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Feature.select().df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple cases as we just saw, we can use files to store datasets.\n",
    "\n",
    "In more complex cases, however, we'd like store collections of images, collections of data objects, or SQL tables in BigQuery, Snowflake, or Postgres.\n",
    "\n",
    "Hence, we need a second central class for data storage: {class}`~lamindb.Dataset`.\n",
    "\n",
    "We'll start with the simplest case: A dataset that's stored in a single file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A single DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at a second batch of the iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "df = ln.dev.datasets.df_iris_in_meter_batch2()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ln.Dataset(df, name=\"Iris flower dataset batch 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataframe back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.load()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` object has a 1:1 correspondence to an underlying file object, accessible via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you can stage the underlying parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.file.stage()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data got added with a storage key based on the `id`, because here, we didn't pass the `key` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ln.File.tree()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the database, we're now seeing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.view()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} Dataset overview\n",
    "\n",
    "Basic dataset metadata is:\n",
    "\n",
    "- `id`: a universal ID that also serves as a primary key in the SQL table\n",
    "- `name`: a name\n",
    "- `hash`: an MD5 hash useful to check for integrity and collisions\n",
    "- `file`: a link to a single file, if the dataset consists in a single file\n",
    "- `files`: a link to several files, if the dataset consists in several files (is \"sharded\")\n",
    "- `created_at`: time of creation\n",
    "- `updated_at`: time of last update\n",
    "- `created_by`: the {class}`~lamindb.User` who created the file\n",
    "\n",
    "Managing the underlying data:\n",
    "\n",
    "- `load()`: load the file to memory for formats like `.parquet`, `.zarr`, and `.h5ad`\n",
    "- `backed()`: the path (cloud or local)\n",
    "\n",
    "For a full reference, see {class}`~lamindb.Dataset`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple DataFrames - sharded datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we measure data in batches and want to store these batches separately.\n",
    "\n",
    "Let us look at how to construct a `Dataset` from two (or more) files corresponding to these batches (or \"shards\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = ln.File.select(description=\"Iris flower dataset batch 1\").one()\n",
    "file2 = dataset.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ln.Dataset.from_files(name=\"The combined Iris dataset\", files=[file1, file2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the sharded dataset as if it was one dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In storage, you see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.tree()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the database, you see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.view()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# generate some files in default storage\n",
    "ln.dev.datasets.generate_cell_ranger_files(\"sample_001\", ln.settings.storage)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass an existing directory to {meth}`~lamindb.File.from_dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ln.File.from_dir(\"./mydata/sample_001/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.save(files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the files as a tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.tree()  # to subset, call ln.File.tree(\"sample_001\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under-the-hood, the following records got written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.select(key__startswith=\"sample_001/\").df().head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query a specific file by passing the full key to `ln.select`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.select(key=\"sample_001/metrics_summary.csv\").df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that LaminDB treats directories similar to S3, as a plain prefix in the storage `key`.\n",
    "\n",
    "If you want to flexibly group files, consider label ({class}`~lamindb.Label`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say, we want to tag the files related to `sample_0001` independent of where they are in storage.\n",
    "\n",
    "Let's create and save a tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ln.Label(name=\"Sample 0001\")\n",
    "label.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now label each file in `files` with this tag and save the update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    file.labels.add(label)\n",
    "ln.save(files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now query by this tag (and arbitrarily more):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.select(labels=label).df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also group tags in a hierarchy using the same principles with which we can manage arbitrary ontologies.\n",
    "\n",
    "Let's create a super class for the sample tag and label our tag with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_label = ln.Label(name=\"Sample\")\n",
    "sample_label.save()\n",
    "label.parents.add(sample_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the hierarchical structure of tags and easily query for all files that have _any_ sample tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.view_parents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.File.select(labels__parents=sample_label).df().head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, update & delete validated metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To end this guide through basic file & metadata tracking, let's see how to update records storing metadata for any entity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create & save records"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ln.Label(name=\"Project A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ln.Label(name=name) for name in [\"Project B\", \"Project C\", \"Project D\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that for every record creation, a search compares whether a similar already exists!\n",
    "    \n",
    "This is to avoid inserting duplicated records.\n",
    "\n",
    "You can switch it off (for performance gains) via `ln.settings.upon_create_search_names = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.save(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if you try to create the same record again, it will load instead of re-creating it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.Label(name=\"Project A\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll learn about more advanced data validation in further guides."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ln.Label.select(name=\"Project A\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.name = \"Project a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ln.Label.select(name=\"Project B\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# clean up what we wrote in this notebook\n",
    "!lamin delete mydata\n",
    "!rm -r mydata\n",
    "!rm paradisi05_laminopathic_nuclei.jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "nbproject": {
   "id": "NJvdsWWbJlZS",
   "parent": null,
   "pypackage": null,
   "time_init": "2023-03-09T13:30:29.239953+00:00",
   "user_handle": "testuser2",
   "user_id": "bKeW4T6E",
   "user_name": "Test User2",
   "version": "0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae1fefc8646a06dd2e75004cd934adda7c5727b046986a772e3b44b0ffba9754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
