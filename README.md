[![Stars](https://img.shields.io/github/stars/laminlabs/lamindb?logo=GitHub)](https://github.com/laminlabs/lamindb)
[![codecov](https://codecov.io/gh/laminlabs/lamindb/branch/main/graph/badge.svg?token=VKMRJ7OWR3)](https://codecov.io/gh/laminlabs/lamindb)
[![Docs](https://img.shields.io/badge/docs-humans-yellow)](https://docs.lamin.ai)
[![DocsLLMs](https://img.shields.io/badge/docs-LLMs-yellow)](https://docs.lamin.ai/summary.md)
[![pypi](https://img.shields.io/pypi/v/lamindb?color=blue&label=pypi%20package)](https://pypi.org/project/lamindb)
[![PyPI Downloads](https://img.shields.io/pepy/dt/lamindb?logo=pypi)](https://pepy.tech/project/lamindb)

# LaminDB - A data lakehouse for biology

LaminDB is an open-source data lakehouse to enable learning at scale in biology.
It lets you track data transformations and organize large collections of biological datasets through validation & annotation to provide context, queries & reproducibility.

Being built on the Django ORM, fsspec, and big data formats, it abstracts interacting with file systems, SQL databases, ontology managers, workflow managers, data lineage trackers, data lakes and warehouses so that you can interact with data through the entities that matter for your research: genes, proteins, experiments, samples, and others that provide the features & labels for iterative learning.

<details>
<summary>Why?</summary>
<br>
Reproducing analytical results or understanding how a dataset or model was created can be a pain.
Leave alone training models on historical data, LIMS & ELN systems, orthogonal assays, or datasets generated by other teams.

Biological datasets are typically managed with versioned storage systems (file systems, object storage, git, dvc), GUI-focused community or SaaS platforms (LIMS & ELN systems), structureless data lakes, rigid data warehouses (SQL, monolithic arrays), and data lakehouses for tabular data.

LaminDB goes further with a lakehouse that models biological datasets beyond tables with enough structure to enable queries and enough freedom to keep the pace of R&D high.

For data structures like `DataFrame`, `AnnData`, `.zarr`, `.tiledbsoma`, etc., LaminDB tracks the rich context that collaborative biological research requires and uses it to validate and index datasets to enable queries. In particular, you get

- data lineage: data sources and transformations; scientists and machine learning models
- domain knowledge and experimental metadata: the features and labels derived from domain entities

In this [blog post](https://blog.lamin.ai/problems), we discuss a breadth of data management problems of the field.

</details>

<details>
<summary>Features</summary>
<br>

- Manage & access data & metadata across any number of storage locations with a unified lakehouse API
- Use distributed array formats in memory & storage: DataFrame, AnnData, MuData, tiledbsoma, ... backed by parquet, zarr, LanceDB, TileDB, HDF5, DuckDB, ...
- Track data lineage across notebooks, scripts, functions & pipelines
- Manage execution reports, source code and compute environments along with built-in data versioning
- Manage registries for experimental metadata & in-house ontologies, providing a LIMS and ontology tool in one
- Validate & annotate datasets
- Zero lock-in: LaminDB runs on generic backends server-side and is _not_ a client for "Lamin Cloud"
  - Flexible storage backends (local, S3, GCP, https, HF, R2, anything fsspec supports)
  - Two SQL backends for managing metadata: SQLite & Postgres
- Scalable: metadata registries support 100s of millions of entries, storage is as scalable as S3
- Plug-in custom modules & manage database schema migrations to build your custom applications
- Auditable: data & metadata records are hashed, timestamped, and attributed to users (full audit log to come)
- Tested, typed, [idempotent](https://docs.lamin.ai/faq/idempotency) & [ACID]((https://docs.lamin.ai/faq/acid)
- Integrations: Vitessce, workflow managers like [redun](redun) & [nextflow](nextflow) annd more

</details>

## Setup

<!-- copied from quick-setup-lamindb.md -->

Install the `lamindb` Python package:

```shell
pip install lamindb
```

Create a LaminDB instance:

```shell
lamin init --storage ./quickstart-data  # or s3://my-bucket, gs://my-bucket
```

Or if you have write access to an instance, connect to it:

```shell
lamin connect account/name
```

## Quickstart

<!-- copied from preface.md -->

Track a script or notebook run with source code, inputs, outputs, logs, and environment.

<!-- copied from py-quickstart.py -->

```python
import lamindb as ln

ln.track()  # track a run
open("sample.fasta", "w").write(">seq1\nACGT\n")
ln.Artifact("sample.fasta", key="sample.fasta").save()  # create an artifact
ln.finish()  # finish the run
```

<!-- from here on, slight deviation from preface.md, where all this is treated in the walk through in more depth -->

This code snippet creates an artifact, which can store a dataset or model as a file or folder in various formats.
Running the snippet as a script (`python create-fasta.py`) produces the following data lineage.

```python
artifact = ln.Artifact.get(key="sample.fasta")  # query artifact by key
artifact.view_lineage()
```

<img src="https://lamin-site-assets.s3.amazonaws.com/.lamindb/EkQATsQL5wqC95Wj0005.png" width="300">

You'll know how that artifact was created and what it's used for ([interactive visualization](https://lamin.ai/laminlabs/lamindata/artifact/8incOOgjn6F0K1TS)) in addition to capturing basic metadata:

```python
artifact.describe()
```

<img src="https://lamin-site-assets.s3.amazonaws.com/.lamindb/BOTCBgHDAvwglN3U0002.png" width="550">

You can organize datasets with validation & annotation of any kind of metadata to then access them via queries & search. Here is a more [comprehensive example](https://lamin.ai/laminlabs/lamindata/artifact/9K1dteZ6Qx0EXK8g).

<img src="https://lamin-site-assets.s3.amazonaws.com/.lamindb/6sofuDVvTANB0f480002.png" width="850">

For instance, to query artifacts by the script that created them, use:

```python
ln.Artifact.filter(transform__key="create-fasta.py").to_dataframe()  # query artifacts by transform key
```

To annotate an artifact with a label, use:

```python
my_experiment = ln.ULabel(name="My experiment").save()  # create a label in the universal label ontology
artifact.ulabels.add(my_experiment)  # annotate the artifact with the label
```

To query for artifacts, use the `filter()` statement.

```python
ln.Artifact.filter(ulabels=my_experiment, suffix=".fasta").to_dataframe()  # query with ulabel and suffix
```

If you have a structured dataset like a `DataFrame`, an `AnnData`, or another array, you can validate the content of the dataset (and parse annotations).
Here is an example for a dataframe: [docs.lamin.ai/introduction#validate-an-artifact](https://docs.lamin.ai/introduction#validate-an-artifact).

With a large body of validated datasets, you can then access data through distributed queries & batch streaming, see here: [docs.lamin.ai/arrays](https://docs.lamin.ai/arrays).

## Docs

Copy [summary.md](https://docs.lamin.ai/summary.md) into an LLM chat and let AI explain or read the [docs](https://docs.lamin.ai).
